import os
import platform
import streamlit as st

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# â”€â”€ 1. config â”€â”€
DOCUMENT_PATH = "C:/Users/Serena Li/OneDrive/Desktop/å¯¦é©—å®¤/team/ã€æ„›å¥åº·â”‚ç†è²¡ç”Ÿæ´»é€šã€‘é™³äº®æ­é†«å¸«è«‡ã€Œä½ çŸ¥é“è‡ªå·±çš„è…¦å¹´é½¡å—ï¼Ÿã€.txt"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
MAX_RETRIEVER_K = 5
DEFAULT_MODEL = "mistral"

# â”€â”€ 2. utils â”€â”€
def setup_chinese_font():
    import matplotlib
    if platform.system() == 'Windows':
        matplotlib.rcParams['font.family'] = 'Microsoft JhengHei'
    elif platform.system() == 'Darwin':
        matplotlib.rcParams['font.family'] = 'Heiti TC'
    else:
        matplotlib.rcParams['font.family'] = 'Noto Sans CJK TC'
    matplotlib.rcParams['axes.unicode_minus'] = False

# â”€â”€ 3. data_loader â”€â”€
@st.cache_data
def load_and_split_documents(path: str):
    loader = TextLoader(path, encoding="utf-8")
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    return splitter.split_documents(docs)

# â”€â”€ 4. index_builder â”€â”€
@st.cache_resource
def build_vectorstore(_docs, collection_name="rag-collection"):
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    return Chroma.from_documents(
        documents=_docs,
        collection_name=collection_name,
        embedding=embeddings
    )

def get_retriever(vs):
    total = len(vs._collection.get()["metadatas"])
    k = min(MAX_RETRIEVER_K, total)
    return vs.as_retriever(search_kwargs={"k": k})

# â”€â”€ 5. rag_pipeline â”€â”€
PROMPT = """è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ä»¥ç¹é«”ä¸­æ–‡ä½œç­”ï¼Œä¸å¾—åŠ å…¥æœªæåŠçš„è³‡è¨Šã€‚

{context}

å•é¡Œï¼š{question}
å›ç­”ï¼š"""
@st.cache_resource
def create_rag_chain(_retriever, model_name: str, temperature: float = 0.0):
    llm = ChatOllama(model=model_name, temperature=temperature)
    prompt = PromptTemplate.from_template(PROMPT)
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=_retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
    )

# â”€â”€ Streamlit UI â”€â”€
st.set_page_config(page_title="RAG Demo", layout="wide")
st.title("ğŸ“š RAG å•ç­”ç³»çµ±")

# 1. æº–å‚™è³‡æº
docs = load_and_split_documents(DOCUMENT_PATH)
vs   = build_vectorstore(docs)
retr = get_retriever(vs)

# 2. å´æ¬„è¨­å®š
model = st.sidebar.selectbox("é¸æ“‡æ¨¡å‹", ["mistral","llama3:8b","gemma:7b","qwen:7b"], index=0)
temp  = st.sidebar.slider("æº«åº¦ (temperature)", 0.0, 1.0, 0.0, 0.01)

# 3. ä½¿ç”¨è€…è¼¸å…¥
question = st.text_area("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š", height=150)
if st.button("ç”¢ç”Ÿå›ç­”"):
    with st.spinner("æ¨¡å‹æ€è€ƒä¸­..."):
        chain = create_rag_chain(retr, model_name=model, temperature=temp)
        res   = chain.invoke({"query": question})
    st.subheader("ğŸ“ å›ç­”")
    st.write(res["result"])
    st.subheader("ğŸ“‘ ä¾æ“šä¾†æº")
    for doc in res["source_documents"]:
        st.write(f"- {doc.metadata.get('source', '')}: {doc.page_content[:200]}â€¦")
