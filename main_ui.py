import os
import platform
import streamlit as st

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# â”€â”€ 1. config â”€â”€
DOCUMENT_DIR = "C:/Users/Serena Li/OneDrive/Desktop/vscode/with_punctuation" 
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
MAX_RETRIEVER_K = 5
DEFAULT_MODEL = "mistral"

# â”€â”€ 2. utils â”€â”€
def setup_chinese_font():
    import matplotlib
    if platform.system() == 'Windows':
        matplotlib.rcParams['font.family'] = 'Microsoft JhengHei'
    elif platform.system() == 'Darwin':
        matplotlib.rcParams['font.family'] = 'Heiti TC'
    else:
        matplotlib.rcParams['font.family'] = 'Noto Sans CJK TC'
    matplotlib.rcParams['axes.unicode_minus'] = False

# â”€â”€ 3. data_loader â”€â”€
@st.cache_data
def load_and_split_documents_from_dir(directory: str):
    all_docs = []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            filepath = os.path.join(directory, filename)
            loader = TextLoader(filepath, encoding="utf-8")
            docs = loader.load()
            all_docs.extend(splitter.split_documents(docs))
    return all_docs

# â”€â”€ 4. index_builder â”€â”€
@st.cache_resource
def build_vectorstore(_docs, collection_name="rag-collection"):
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    return Chroma.from_documents(
        documents=_docs,
        collection_name=collection_name,
        embedding=embeddings
    )

def get_retriever(vs):
    total = len(vs._collection.get()["metadatas"])
    k = min(MAX_RETRIEVER_K, total)
    return vs.as_retriever(search_kwargs={"k": k})

# â”€â”€ 5. rag_pipeline â”€â”€
PROMPT = """è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹å®Œå…¨ä»¥ç¹é«”ä¸­æ–‡ä½œç­”ï¼Œä¸å¯è‡ªè¡ŒåŠ å…¥æœªæåŠçš„è³‡è¨Šã€‚

{context}

å•é¡Œï¼š{question}
å›ç­”ï¼š"""
@st.cache_resource
def create_rag_chain(_retriever, model_name: str):
    llm = ChatOllama(model=model_name)
    prompt = PromptTemplate.from_template(PROMPT)
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=_retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
    )

# â”€â”€ Streamlit UI â”€â”€
st.set_page_config(page_title="RAG Demo", layout="wide")
st.title("ğŸ“š å¥åº·çŸ¥è­˜å•ç­”ç³»çµ±")

# 1. æº–å‚™è³‡æº
docs = load_and_split_documents_from_dir(DOCUMENT_DIR)
vs   = build_vectorstore(docs)
retr = get_retriever(vs)

# 2. å´æ¬„è¨­å®š
model = st.sidebar.selectbox("é¸æ“‡æ¨¡å‹", ["mistral", "llama3:8b", "gemma:7b", "taide-medicine-qa-tw-q6"], index=0)

# 3. ä½¿ç”¨è€…è¼¸å…¥
question = st.text_area("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š", height=150)
if st.button("ç”¢ç”Ÿå›ç­”"):
    with st.spinner("æ¨¡å‹æ€è€ƒä¸­..."):
        chain = create_rag_chain(retr, model_name=model)
        res   = chain.invoke({"query": question})
    st.subheader("ğŸ“ å›ç­”")
    st.write(res["result"])
    if res["source_documents"]:
        st.subheader("ğŸ“‘ ä¾æ“šä¾†æº")
        used_files = set()
        for doc in res["source_documents"]:
            path = doc.metadata.get('source', '')
            filename = os.path.splitext(os.path.basename(path))[0]
            used_files.add(filename)
        for name in sorted(used_files):
            st.write(f"- {name}")

