import os
import platform
import ollama
import pandas as pd
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from rouge_score import rouge_scorer
from bert_score import score

# â”€â”€ åŸºæœ¬è¨­å®š â”€â”€
DOCUMENT_PATH = "C:/Users/Serena Li/OneDrive/Desktop/å¯¦é©—å®¤/team/ã€æ„›å¥åº·â”‚ç†è²¡ç”Ÿæ´»é€šã€‘é™³äº®æ­é†«å¸«è«‡ã€Œä½ çŸ¥é“è‡ªå·±çš„è…¦å¹´é½¡å—ï¼Ÿã€.txt"
QUESTION_PATH = "C:/Users/Serena Li/OneDrive/Desktop/å¯¦é©—å®¤/team/test.txt"  # æ ¼å¼ï¼šæ¯è¡Œç‚ºã€Œå•é¡Œï¼Ÿæ­£ç¢ºç­”æ¡ˆã€
MODELS = ["mistral", "llama3:8b", "gemma:7b", "taide-medicine-qa-tw-q6"]
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
MAX_RETRIEVER_K = 5
PROMPT = """è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ä»¥ç¹é«”ä¸­æ–‡ä½œç­”ï¼Œä¸å¾—åŠ å…¥æœªæåŠçš„è³‡è¨Šã€‚\n\n{context}\n\nå•é¡Œï¼š{question}\nå›ç­”ï¼š"""

# â”€â”€ è³‡æ–™è¼‰å…¥èˆ‡å‘é‡åŒ– â”€â”€
def load_and_split_documents(path: str):
    docs = TextLoader(path, encoding="utf-8").load()
    return RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP).split_documents(docs)

def build_vectorstore(docs, collection_name="rag-collection"):
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    return Chroma.from_documents(docs, collection_name=collection_name, embedding=embeddings)

def get_retriever(vs):
    k = min(MAX_RETRIEVER_K, len(vs._collection.get()["metadatas"]))
    return vs.as_retriever(search_kwargs={"k": k})

def create_rag_chain(retriever, model_name: str):
    llm = ChatOllama(model=model_name, temperature=0.0)
    prompt = PromptTemplate.from_template(PROMPT)
    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff", chain_type_kwargs={"prompt": prompt})

# â”€â”€ å•ç­”è¼‰å…¥ â”€â”€
def load_questions(filepath):
    qas = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            if "?" in line:
                q, a = line.strip().split("?")
                qas.append((q + "?", a.strip()))
    return qas

# â”€â”€ è©•ä¼°æŒ‡æ¨™ â”€â”€
def compute_metrics(pred, ref):
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    r = rouge.score(ref, pred)
    P, R, F1 = score([pred], [ref], lang="zh", verbose=False)
    return {
        "rouge1": r['rouge1'].fmeasure,
        "rouge2": r['rouge2'].fmeasure,
        "rougeL": r['rougeL'].fmeasure,
        "bert_f1": F1[0].item()
    }

# â”€â”€ ä¸»æµç¨‹ â”€â”€
def evaluate_all():
    docs = load_and_split_documents(DOCUMENT_PATH)
    vs = build_vectorstore(docs)
    retr = get_retriever(vs)
    qas = load_questions(QUESTION_PATH)

    results = []
    for model in MODELS:
        print(f"ğŸ” Evaluating model: {model}")
        chain = create_rag_chain(retr, model)
        for q, gold in qas:
            res = chain.invoke({"query": q})
            pred = res["result"]
            metrics = compute_metrics(pred, gold)
            results.append({
                "model": model,
                "question": q,
                "reference": gold,
                "prediction": pred,
                **metrics
            })

    df = pd.DataFrame(results)
    df.to_csv("rouge_bert_evaluation.tsv", sep="\t", index=False)
    print("âœ… è©•ä¼°çµæœå·²å„²å­˜è‡³ rouge_bert_evaluation.tsv")

if __name__ == "__main__":
    evaluate_all()
